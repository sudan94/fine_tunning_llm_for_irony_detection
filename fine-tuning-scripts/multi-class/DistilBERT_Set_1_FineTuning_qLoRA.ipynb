{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smZAvuwTRs3v","outputId":"59bbaead-22f0-411a-ebe3-d746743bfeb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub>=0.21.2 (from datasets)\n","  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n","Collecting peft\n","  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.40.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Collecting accelerate>=0.21.0 (from peft)\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"]}],"source":["!pip install datasets\n","!pip install peft\n","!pip install evaluate\n","!pip install -U \"huggingface_hub[cli]\"\n","! pip install -U accelerate\n","! pip install -U transformers\n","! pip install -U bitsandbytes\n","! pip install accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHI_3okURigd"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict, Dataset\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    BitsAndBytesConfig,\n","    Trainer)\n","\n","from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n","import evaluate\n","import torch\n","import numpy as np\n","import bitsandbytes as bnb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0HMOxSX0A-M"},"outputs":[],"source":["!huggingface-cli login"]},{"cell_type":"markdown","metadata":{"id":"glO5VLLAV6kj"},"source":["# dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRnSWdB1g4ah"},"outputs":[],"source":["# sst2\n","# The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. It uses the two-way (positive/negative) class split, with only sentence-level labels.\n","# dataset = load_dataset('csv', data_dir='/sem.csv', split='train')\n","dataset = load_dataset(\"sudan94/SemEvalEmoji2018\")\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wnfwkc9VSVHc"},"outputs":[],"source":["# display % of training data with label=1\n","np.array(dataset['train']['label']).sum()/len(dataset['train']['label'])"]},{"cell_type":"markdown","metadata":{"id":"9w7RcqFVV-hI"},"source":["# model"]},{"cell_type":"markdown","metadata":{"id":"EDSYlcyfxR-r"},"source":["In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained method.\n","\n","AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary:\n","\n","Instantiating one of AutoModel, AutoConfig and AutoTokenizer will directly create a class of the relevant architecture (ex: model = AutoModel.from_pretrained('bert-base-cased') will create a instance of BertModel)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptr7HbwqSsa0"},"outputs":[],"source":["model_checkpoint = 'distilbert-base-uncased'\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit =True,\n","    load_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n",")\n","\n","# define label maps\n","id2label = {0: \"non_ironic\", 1: \"verbal_irony_polarity_contrast\",2:\"situational_irony\",3:\"verbal_irony\"}\n","label2id = {\"non_ironic\":0, \"verbal_irony_polarity_contrast\":1,\"situational_irony\":2,\"verbal_irony\":3}\n","\n","# generate classification model from model_checkpoint\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=4, id2label=id2label, label2id=label2id,\n","    trust_remote_code=True,\n","     load_in_8bit=False,\n","      torch_dtype=torch.float32,\n","    quantization_config=bnb_config,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vnFth_CS0Ec"},"outputs":[],"source":["# display architecture\n","model"]},{"cell_type":"markdown","metadata":{"id":"a8az9-hQWCFo"},"source":["# preprocess data"]},{"cell_type":"markdown","metadata":{"id":"q9EG4dxxyGZy"},"source":["Tokenization is a critical first step in preparing data for Large Language Models (LLMs) because these models don't understand raw text; they process numerical data. The tokenizer's role is to convert text into numbers that the model can understand."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpL518OqS3ik"},"outputs":[],"source":["# create tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, padding_side=\"left\")\n","\n","# add pad token if none exists\n","# Pad Token (pad_token): In NLP, padding is used to ensure that all sequences (like sentences or paragraphs) are of the same length when feeding them into a model.\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6TUa-tWS6k-"},"outputs":[],"source":["# create tokenize function\n","def tokenize_function(examples):\n","    # extract text\n","    text = examples[\"tweet\"]\n","\n","    #tokenize and truncate text\n","    tokenizer.truncation_side = \"left\"\n","    tokenized_inputs = tokenizer(\n","        text, #actual text\n","        return_tensors=\"np\", # return type numpy array\n","        truncation=True, #Indicates that truncation should be applied based on the specified parameters\n","        max_length=512 #Specifies the maximum length of the tokenized sequence\n","    )\n","\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sR3wMZh8S8qN"},"outputs":[],"source":["# tokenize training and validation datasets\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","tokenized_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sx23oGyS-7t"},"outputs":[],"source":["# create data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"t3ja7mzQWFSP"},"source":["# evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVWbfqUOTDgd"},"outputs":[],"source":["# import accuracy evaluation metric\n","accuracy = evaluate.load(\"accuracy\")\n","f1_metric = evaluate.load(\"f1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgmOWB1tTF8N"},"outputs":[],"source":["# define an evaluation function to pass into trainer later\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=1)\n","    accuray = accuracy.compute(predictions=predictions, references=labels)\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n","    # print(accuracy)\n","\n","    return {\"accuray\": accuray['accuracy'], \"f1\": f1['f1']}\n","\n","    # return {\"accuracy\":accuracy.compute(predictions=predictions, references=labels), \"f1\":f1_metric.compute(predictions=predictions, references=labels,average=\"macro\")}"]},{"cell_type":"markdown","metadata":{"id":"89YKx-izWITj"},"source":["# Apply untrained model to text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0_AokiaTIL-"},"outputs":[],"source":["# define list of examples\n","text_list = dataset[\"test\"][\"tweet\"]\n","ground_truth = dataset[\"test\"][\"label\"]\n","# Initialize a list to store the table data\n","table_data = []\n","i = 0\n","for text in text_list:\n","    # tokenize text\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n","    # compute logits\n","    logits = model(inputs).logits\n","    # convert logits to label\n","    predictions = torch.argmax(logits)\n","    # print(predictions.tolist())\n","\n","     # Get the label string\n","    predicted_label = id2label[predictions.tolist()]\n","    # Append the text and predicted label to the table data\n","    table_data.append([text, predicted_label,predictions.tolist(), ground_truth[i]])\n","    i+=1\n","\n","    # print(text + \" - \" + id2label[predictions.tolist()])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YotA_FtEotz7"},"outputs":[],"source":["# print(table_data)\n","import pandas as pd\n","df = pd.DataFrame(table_data,columns =['tweet','predicted_class','predicted_label','actual_label'])\n","df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxVQNb7IQEXX"},"outputs":[],"source":["# from tabulate import tabulate\n","# print(\"Untrained model predictions:\")\n","# print(\"----------------------------\")\n","# print(tabulate(table_data, headers=[\"tweet\", \"predicted_class\", \"predicted_label\" ,\"actual_label\"], tablefmt=\"grid\"))"]},{"cell_type":"markdown","metadata":{"id":"wJrM1bO8WLte"},"source":["# Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEWCGKLbTLxV"},"outputs":[],"source":["peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n","                        r=64,\n","                        lora_alpha=16,\n","                        lora_dropout=0.1, # dropot rate for avoiding overfitting\n","                        target_modules = ['q_lin'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEFKNTLKTPAl"},"outputs":[],"source":["peft_config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaFePuyZTQil"},"outputs":[],"source":["model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWHSqHIITSNt"},"outputs":[],"source":["# hyperparameters\n","lr = 1e-3\n","# lr = 1e-4 #default learning rate\n","batch_size = 16\n","num_epochs = 10\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IjsJpqDTUHO"},"outputs":[],"source":["# define training arguments\n","training_args = TrainingArguments(\n","    output_dir= model_checkpoint + \"-lora-text-classification\",\n","    logging_steps = 231,\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model = 'eval_f1'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtM0djTaTWHV"},"outputs":[],"source":["# creater trainer object\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# train model\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"5Tg5iEnNWQsm"},"source":["# Generate prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_7pWg1esNJv"},"outputs":[],"source":["best_checkpoint = trainer.state.best_model_checkpoint\n","best_checkpoint\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8GLoYXw2CfN"},"outputs":[],"source":["trainer.state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9s4KQfExqRb"},"outputs":[],"source":["best_model = AutoModelForSequenceClassification.from_pretrained(best_checkpoint, num_labels=4, id2label=id2label, label2id=label2id)\n","best_model\n","# test = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-lora-text-classification/checkpoint-2079', num_labels=4, id2label=id2label, label2id=label2id)\n","# test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IHlDI5GY1Ifl"},"outputs":[],"source":["model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3i0MiZm2Vqi9"},"outputs":[],"source":["model.to('cpu')\n","# Initialize a list to store the table data\n","table_data = []\n","i=0\n","print(\"Trained model predictions:\")\n","print(\"--------------------------\")\n","for text in text_list:\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n","\n","    logits = model(inputs).logits\n","    predictions = torch.max(logits,1).indices\n","\n","    # Get the label string\n","    predicted_label = id2label[predictions.tolist()[0]]\n","    # Append the text and predicted label to the table data\n","    table_data.append((text, predicted_label,predictions.tolist()[0], ground_truth[i]))\n","    i+=1\n","\n","    # print(text + \" - \" + id2label[predictions.tolist()[0]])\n","tuned_df = pd.DataFrame(table_data,columns =['tweet','predicted_class','predicted_label','actual_label'])\n","tuned_df.head(5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBnMmWSruSEy"},"outputs":[],"source":["nottuned = \"nottuned_result_distillbert_set1_lora.csv\"\n","finetuned = \"finetuned_result_distillbert_set1_lora.csv\"\n","\n","df.to_csv(nottuned,  encoding='utf-8')\n","tuned_df.to_csv(finetuned, encoding='utf-8')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hV89O5YuXwPE"},"outputs":[],"source":["# model.save_pretrained('fine_tuned_model')"]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1KFwEQ0FugYT5BqMZAfBapcfvjCnYxeIg","timestamp":1703599828744},{"file_id":"https://github.com/fshnkarimi/Fine-tuning-an-LLM-using-LoRA/blob/main/FineTuning_LoRA.ipynb","timestamp":1703591421801}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}